我们继续保持清晰、直接的风格，来深入探讨一个在实际工作中非常常见，也颇为头疼的问题：MySQL 的优化器为什么有时候会“犯傻”，选错索引？

-----

我们都知道，当一张表有多个索引时，具体使用哪个索引来执行查询，是由 MySQL 的\*\*优化器（Optimizer）\*\*来决定的。优化器的目标很单纯：**找出它认为成本最低、执行效率最高的方案**。

但“理想”与“现实”之间常常有差距。我们来看一个具体的案例，理解优化器是如何做出判断，以及它为什么会犯错。

### 1\. 问题的引入：一次意外的全表扫描

文章中构造了一个场景，我们简化一下它的核心操作：

1.  一张有10万行数据的表 `t`，在 `a` 和 `b` 字段上分别有索引。
2.  执行查询 `select * from t where a between 10000 and 20000;`
3.  正常情况下，`EXPLAIN` 显示，优化器正确地使用了索引 `a`。
4.  但在一个特殊操作（在一个长事务中，删除全表数据再重新插入）之后，再次执行同样的查询，优化器居然放弃了索引 `a`，选择了**全表扫描**，导致查询时间从21毫秒飙升到40毫秒。

为什么会这样？

### 2\. 优化器的决策依据：成本估算

优化器选择索引，不是凭感觉，而是基于一个**成本模型**。它会估算不同执行方案的“成本”，然后选择成本最低的那个。影响成本的主要因素有：

  * **I/O成本**：即读取数据页的成本。读取的页越少，成本越低。这通常与**扫描行数**成正比。
  * **CPU成本**：处理数据、进行计算和比较的成本。

对于一个查询，优化器在执行前并不知道精确的扫描行数，它只能依赖**统计信息（Statistics）来进行估算**。

#### 统计信息是怎么来的？

这个统计信息的核心指标，就是索引的**基数（Cardinality）**，即一个索引上不重复值的数量。基数越大，说明索引的选择性越好，区分度越高。

MySQL 计算基数，采用的是**采样统计**的方法：

1.  `InnoDB` 默认会随机选择 N 个数据页。
2.  统计这 N 个页面上不同值的数量。
3.  用这个平均数乘以索引的总页面数，得出一个估算的基数。

因为是采样，所以这个结果**天生就可能不准**。

### 3\. 案例一分析：为什么会选错索引？

在文章的例子中，执行 `DELETE` + `INSERT` 操作后，索引 `a` 的统计信息变得**严重不准确**。`EXPLAIN` 显示，优化器估算走索引 `a` 需要扫描37000多行，而实际上只需要扫描10001行。

这时，优化器会比较两个方案的成本：

  * **方案一（走索引a）**：

    1.  扫描索引 `a`，估算需要访问37000多个索引页。
    2.  每拿到一个索引 `a` 的值，都要根据主键ID**回表**到主键索引去捞取整行数据。
    3.  总成本 = I/O成本(扫描37k行索引) + CPU成本(比较) + **I/O成本(37k次回表)**

  * **方案二（走全表扫描）**：

    1.  直接扫描主键索引，顺序地读取10万行数据。
    2.  不需要回表。
    3.  总成本 = I/O成本(扫描10w行主键) + CPU成本(比较)

在优化器看来，虽然方案二扫描的行数更多，但它**避免了大量的回表操作**。基于它错误的行数估算，它计算出方案二的总成本反而低于方案一，于是做出了“全表扫描”这个看似愚蠢、实则“情有可原”的错误决定。

### 4\. 案例二分析：为什么不选扫描行数少的索引？

文章中第二个例子 `select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1;` 更加经典。

  * 走索引 `a`：只需要扫描1000行，但结果需要按 `b` 排序。
  * 走索引 `b`：需要扫描50001行，但因为索引 `b` 本身就是按 `b` 有序的，所以**结果天然有序，无需额外排序**。

优化器再次进行了成本估算，它认为“扫描5万行但省去排序”的成本，比“扫描1000行再额外进行一次文件排序”的成本要低。于是，它又一次选择了扫描行数更多的索引 `b`。

### 5\. 如何应对优化器的“犯傻”？

当发现优化器选错索引时，我们有几种处理方法：

1.  **方法一：`ANALYZE TABLE`**
    这是最直接的方法。如果怀疑是统计信息不准，可以执行 `analyze table t;` 命令。这会强制数据库重新采样并计算统计信息。很多时候，更新完统计信息，优化器就“恢复正常”了。

2.  **方法二：`FORCE INDEX`**
    在 SQL 语句中，可以用 `force index(索引名)` 来“命令”优化器必须使用你指定的索引。

    ```sql
    select * from t force index(a) where a between 10000 and 20000;
    ```

      * **优点**：简单粗暴，立刻见效，可以快速解决线上问题。
      * **缺点**：代码不优雅；如果未来索引改名或删除，代码会报错；缺乏移植性。

3.  **方法三：修改 SQL，引导优化器**
    这是一种更巧妙的方法。比如在案例二中，通过将 `order by b` 改为 `order by b, a`，使得优化器认为两个方案都需要排序，于是扫描行数就成了主要判断依据，从而选择了索引 `a`。这种方法需要对优化器的决策逻辑有较深的理解，不具备通用性。

4.  **方法四：修改或删除索引**
    在某些情况下，可以考虑新建一个更合适的索引，或者与业务方沟通后，删除那个总是被优化器误用的、且非必要的索引。

-----

### 结尾问题解析

> 前面我们在构造第一个例子的过程中，通过 session A 的配合，让 session B 删除数据后又重新插入了一遍数据，然后就发现 explain 结果中，rows 字段从 10001 变成 37000 多。而如果没有 session A 的配合，只是单独执行 `delete from t`、`call idata()`、`explain` 这三句话，会看到 rows 字段其实还是 10000 左右。这是什么原因呢？

**原因在于 `InnoDB` 的事务可见性（MVCC）与索引统计更新机制的相互影响。**

1.  MySQL 的索引统计信息更新，可能是在数据变更达到一定阈值时自动触发，也可能是手动执行 `ANALYZE TABLE`。
2.  这个“统计”的过程，本身也是一个“读”操作，因此它也必须遵守 MVCC 的规则。

**当有 `Session A` 这个长事务存在时：**

  * `Session A` 启动时，创建了一个 `Read View`（一致性读视图），这个视图可以看到 `DELETE` 之前的旧数据。
  * `Session B` 在自己的事务里执行 `DELETE` 和 `INSERT`。
  * 此时，如果触发了索引统计的更新，**这个统计线程看到的数据库，和 `Session A` 看到的是一样的**，因为它也受 `Session A` 这个最老的 `Read View` 的影响。
  * 因此，统计线程**看不到 `Session B` 新插入的10万行数据**。它采样的结果，是基于一个“几乎为空”的表的，但可能还残留着一些旧版本的信息，最终导致它估算出了一个错误的、偏大的行数（比如它可能认为数据分布非常稀疏）。

**当没有 `Session A` 存在时：**

  * `delete from t` 是一个事务，执行完就提交了。
  * `call idata()` 是一个事务，执行完也提交了。
  * 当 `explain` 或者索引统计触发时，数据库中不存在任何旧的事务视图。它看到的是一个实实在在的、包含了10万行新数据的表。
  * 此时的采样统计，是基于真实、完整的数据的，所以估算出来的行数就相对准确。

简单来说，**`Session A` 的长事务，就像给数据库戴上了一副“老花镜”，导致索引统计这个“视力检查”过程，看到了一个过时的、不准确的“幻象”，从而得出了错误的验光结果（统计信息）。**